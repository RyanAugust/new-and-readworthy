{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from lxml import etree\n",
    "import settings\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils(object):\n",
    "    def __init__(self, db_location=settings.db_location):\n",
    "        self.db_location = db_location\n",
    "        \n",
    "    def _connect_to_db(self):\n",
    "        \"\"\"Returns connection to package sqlite db, which holds all source urls \n",
    "        as well as articles which have been previously uplaoded to Instapaper by the application\"\"\"\n",
    "        con = sqlite3.connect(self.db_location)\n",
    "        return con\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inbound_local(utils):\n",
    "    def __init__(self, db_location=settings.db_location):\n",
    "        self.db_location = db_location\n",
    "    \n",
    "    def _gather_sources(self):\n",
    "        \"\"\"Gathers active sources from the package db which will be referenced\n",
    "        by the application when seeking newly published articles\"\"\"\n",
    "        source_query = \"\"\"SELECT name, connect_point, type FROM sources WHERE status=1;\"\"\"\n",
    "        con = self._connect_to_db()\n",
    "        c = con.cursor()\n",
    "        \n",
    "        source_dict = {}\n",
    "        for row in c.execute(source_query):\n",
    "            name, connect_point, _type = row\n",
    "            if _type in source_dict.keys():\n",
    "                source_dict[_type].append((name,connect_point))\n",
    "            else:\n",
    "                source_dict[_type] = [(name,connect_point)]\n",
    "        con.commit()\n",
    "        con.close()\n",
    "        return source_dict\n",
    "\n",
    "    def _rss_gather(self, source_name, rss_feed_link):\n",
    "        \"\"\"Pull all articles given a rss feed link\n",
    "        Details that are passed back will mirror those required for insertion into the articles table\n",
    "        (name, source, connect_point, publish_time)\"\"\"\n",
    "        tree = etree.fromstring(requests.get(rss_feed_link).text)\n",
    "        articles = tree.findall('.//item')\n",
    "\n",
    "        articles_details = []\n",
    "        status = 0\n",
    "        _type = 'RSS'\n",
    "        for article in articles:\n",
    "            articles_details.append((article.find('title').text, source_name,\n",
    "                                     article.find('link').text, article.find('pubDate').text))\n",
    "        return articles_details\n",
    "\n",
    "    def _store_article_details(self):\n",
    "        \"\"\"First checks that there are articles waiting to be written into the local db\n",
    "        then connects and inserts each article into the `articles` table\"\"\"\n",
    "        \n",
    "        assert len(self.collected_articles) > 0, \"No articles collected.\"\n",
    "        \n",
    "        con = self._connect_to_db()\n",
    "        c = con.cursor()\n",
    "        insert_query = \"\"\"INSERT INTO articles (name, source, connect_point, publish_time, insert_time, uploaded)\n",
    "        VALUES(?,?,?,?,?,?)\"\"\"\n",
    "        uploaded = False\n",
    "        insert_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:00')\n",
    "        \n",
    "        article_insert = []\n",
    "        for article in self.collected_articles:\n",
    "            name, source, connect_point, publish_time = article\n",
    "            article_insert.append((name, source, connect_point, publish_time, insert_time, uploaded))\n",
    "        c.executemany(insert_query, article_insert)\n",
    "        [print('|| {} ||'.format(' || '.join([str(x) for x in row]))) for row in article_insert]\n",
    "        con.commit()\n",
    "        con.close()\n",
    "        return 0\n",
    "                        \n",
    "    def check_sources(self):\n",
    "        source_dict = self._gather_sources()\n",
    "        article_details = []\n",
    "        if 'RSS' in source_dict.keys():\n",
    "            for source in source_dict['RSS']:\n",
    "                source_name, rss_feed_link = source\n",
    "                article_details += self._rss_gather(source_name=source_name,\n",
    "                                                        rss_feed_link=rss_feed_link)\n",
    "                print('Sourced: {source_name}'.format(source_name=source_name))\n",
    "        elif 'Medium' in source_dict.keys():\n",
    "            for source in source_dict['Medium']:\n",
    "                source_name, medium_link = source\n",
    "                article_details += self._medium_gather(source_name=source_name,\n",
    "                                                           medium_link=medium_link)\n",
    "        self.collected_articles = article_details\n",
    "        return 0\n",
    "    \n",
    "    def add_source(self, name, _type, connect_point, status):\n",
    "        con = sqlite3.connect(self.db_location)\n",
    "        c = con.cursor()\n",
    "        insert_query = \"\"\"INSERT INTO sources (name, type, connect_point, status, insert_time)\n",
    "                VALUES(?,?,?,?,?)\"\"\"\n",
    "        c.execute(\n",
    "                      insert_query, (name, _type, connect_point, status, \n",
    "                      datetime.datetime.now().strftime('%Y-%m-%d %H:%M:00'))\n",
    "                     )\n",
    "        con.commit()\n",
    "        con.close()\n",
    "        return 0\n",
    "\n",
    "    def _turn_off_source(self, source_name):\n",
    "        con = sqlite3.connect(self.db_location)\n",
    "        c = con.cursor()\n",
    "        off_query = \"\"\"UPDATE sources SET status=0 WHERE name=?\"\"\"\n",
    "        c.execute(off_query, (source_name,))\n",
    "        con.commit()\n",
    "        con.close()\n",
    "        print('{} turned off'.format(source_name))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train = inbound_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Upshot turned off\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train._turn_off_source(source_name='The Upshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sourced: Paul Krugman\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train.check_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Krugman\t| RSS\t| http://www.nytimes.com/svc/collections/v1/publish/www.nytimes.com/column/paul-krugman/rss.xml\t| 1\t| 2019-10-24 16:39\n",
      "The Upshot\t| RSS\t| https://rss.nytimes.com/services/xml/rss/nyt/Upshot.xml\t| 0\t| 2019-10-25 13:33:00\n"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect(settings.db_location)\n",
    "c = con.cursor()\n",
    "for row in c.execute(\"SELECT * FROM sources;\"):\n",
    "    print('\\t| '.join([str(x) for x in row]))\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Debt, Doomsayers and Double Standards || Paul Krugman || https://www.nytimes.com/2019/10/28/opinion/us-budget-deficit.html || Mon, 28 Oct 2019 22:30:08 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| The Day the Trump Boom Died || Paul Krugman || https://www.nytimes.com/2019/10/24/opinion/trump-economy.html || Thu, 24 Oct 2019 20:35:56 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Can Warren Escape the Medicare Trap? || Paul Krugman || https://www.nytimes.com/2019/10/21/opinion/warren-medicare-for-all.html || Mon, 21 Oct 2019 22:30:05 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Democrats, Avoid the Robot Rabbit Hole || Paul Krugman || https://www.nytimes.com/2019/10/17/opinion/democrats-automation.html || Thu, 17 Oct 2019 21:53:37 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| God Is Now Trumpâ€™s Co-Conspirator || Paul Krugman || https://www.nytimes.com/2019/10/14/opinion/trump-william-barr-speech.html || Mon, 14 Oct 2019 22:30:07 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Luckily, Trump Is an Unstable Non-Genius || Paul Krugman || https://www.nytimes.com/2019/10/10/opinion/donald-trump.html || Thu, 10 Oct 2019 22:30:08 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| The Education of Fanatical Centrists || Paul Krugman || https://www.nytimes.com/2019/10/07/opinion/republicans-trump-moderates.html || Mon, 07 Oct 2019 22:00:08 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Here Comes the Trump Slump || Paul Krugman || https://www.nytimes.com/2019/10/03/opinion/trump-economy.html || Thu, 03 Oct 2019 21:30:07 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Warren Versus the Petty Plutocrats || Paul Krugman || https://www.nytimes.com/2019/09/30/opinion/elizabeth-warren-wealth-tax.html || Mon, 30 Sep 2019 22:00:07 GMT || 2019-10-30 17:05:00 || False ||\n",
      "|| Impeaching Trump Is Good for the Economy || Paul Krugman || https://www.nytimes.com/2019/09/26/opinion/trump-economy.html || Thu, 26 Sep 2019 22:50:26 GMT || 2019-10-30 17:05:00 || False ||\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train._store_article_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class outbound_express(object):\n",
    "    def __init__(self, db_location=settings.db_location):\n",
    "        self.db_location = db_location\n",
    "        self.instapaper_account_token = settings.credentials['instapaper']['API_TOKEN']\n",
    "        \n",
    "    def check_upload_needs(self, source_name):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(settings.db_location)\n",
    "c = con.cursor()\n",
    "c.execute(\"DELETE FROM articles;\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
